{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofakduman/Acme-Hired-Web-App/blob/main/lstm_forecasting_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOrppxDnXThw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMRCeG_OYIK8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/LSTM_forecasting/all_data.csv\")\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace(\"Not available\", pd.NA, inplace=True)\n",
        "len(df)\n"
      ],
      "metadata": {
        "id": "zJQVw4GGW5iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped = df.drop(['Conditions', 'Assumed', 'Max.rate(logc.conc / h)', 'Food Name'], axis=1)\n",
        "df_dropped.head()\n",
        "len(df_dropped.dropna())"
      ],
      "metadata": {
        "id": "CfQ_9iRZW_5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped.dropna(inplace=True)\n",
        "len(df_dropped)\n",
        "cleaned_df = df_dropped.drop_duplicates(subset='Record ID', keep='first')"
      ],
      "metadata": {
        "id": "Ln0AQcQiXFZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVEWZXTnZQqs"
      },
      "source": [
        "### logcs nan olanlari at, record ID tekrarlanmasin\n",
        "### Food name kolonunu kaldir\n",
        "### Temperature Nan lari kaldir ve temprature kolonundaki logcs degerleri gibi olan degerleri kaldir deger int e cevriliyorsa kalsin\n",
        "### Nanlari kaldir Aw deki\n",
        "### Assumed kolonunu kaldir\n",
        "### Conditions kolonunu kaldir\n",
        "### Max.rate(logc.conc / h) kolonunu kaldir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSUUTBewYpqE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us7PxDgRaS5E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6lNY4VodF20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfSjI3OIiwo7"
      },
      "outputs": [],
      "source": [
        "def analyze_logcs(logcs_str):\n",
        "    # Değerleri ; ile böl ve float'a çevir\n",
        "    values = list(map(float, logcs_str.split(';')))\n",
        "\n",
        "    # Zaman ve populasyon değerlerini ayır\n",
        "    times = values[::2]\n",
        "    populations = values[1::2]\n",
        "\n",
        "    # Populasyon değerlerinin artıp artmadığını kontrol et\n",
        "    increases = 0\n",
        "    decreases = 0\n",
        "    for i in range(1, len(populations)):\n",
        "        if populations[i] > populations[i-1]:\n",
        "            increases += 1\n",
        "        elif populations[i] < populations[i-1]:\n",
        "            decreases += 1\n",
        "\n",
        "    # Toplam değer sayısı ve artma/azalma sayılarına göre bir sonuç döndür\n",
        "    return {\n",
        "        \"total_values\": len(populations),\n",
        "        \"increases\": increases,\n",
        "        \"decreases\": decreases\n",
        "    }\n",
        "\n",
        "# `Logcs` kolonunu analiz edelim\n",
        "cleaned_df_copy = cleaned_df.copy()\n",
        "results = cleaned_df['Logcs'].apply(analyze_logcs)\n",
        "\n",
        "# Sonuçları yeni kolonlarda saklayalım\n",
        "cleaned_df_copy['Total Values'] = results.apply(lambda x: x[\"total_values\"])\n",
        "cleaned_df_copy['Increases'] = results.apply(lambda x: x[\"increases\"])\n",
        "cleaned_df_copy['Decreases'] = results.apply(lambda x: x[\"decreases\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VszhhJ0hjtFE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "# Veri setinizi yükleyin\n",
        "# cleaned_df_copy = pd.read_csv('your_dataset.csv')  # Örnek veri seti yolu\n",
        "# Sayısal sütunları seçin (örnek olarak sadece sayısal verileri içeren sütunlar listelenmiştir)\n",
        "numeric_columns = ['Temperature (C)', 'Aw', 'pH', 'Total Values', 'Increases', 'Decreases']\n",
        "for col in numeric_columns:\n",
        "    # Sayısal olmayan verileri ve NaN değerleri temizle\n",
        "    cleaned_data = cleaned_df_copy[col].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "    squared_data = cleaned_data ** 2\n",
        "\n",
        "    # Histogramı çiz\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(cleaned_data, kde=True)\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    # Temel İstatistiksel Özellikler\n",
        "    print(f\"\\n{col} - Statistical Species:\")\n",
        "    print(f\"Min: {cleaned_data.min()}\")\n",
        "    print(f\"Max: {cleaned_data.max()}\")\n",
        "    print(f\"Mean: {cleaned_data.mean()}\")\n",
        "    print(f\"Median: {cleaned_data.median()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvkzspI4kNnf"
      },
      "outputs": [],
      "source": [
        "trashold_sample_value = 5\n",
        "df = cleaned_df_copy[cleaned_df_copy['Total Values'] > trashold_sample_value]\n",
        "df = df[df['Temperature (C)'].apply(lambda x: ';' not in x)]\n",
        "df = df[df['pH'].apply(lambda x: ';' not in x)]\n",
        "df = df[df['Aw'].apply(lambda x: ';' not in x)]\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sxmdb8MkOqI"
      },
      "outputs": [],
      "source": [
        "def parse_logcs(logcs_str):\n",
        "    pairs = logcs_str.split(';')\n",
        "    parsed_pairs = []\n",
        "    for i in range(0, len(pairs) - 1, 2):\n",
        "        try:\n",
        "            # Zaman değerini tam sayıya dönüştür\n",
        "            time = float(pairs[i])\n",
        "            # Değer olarak ondalık sayı kullan\n",
        "            value = float(pairs[i + 1])\n",
        "            parsed_pairs.append((time, value))\n",
        "        except ValueError:\n",
        "            # Geçersiz dönüşüm olduğunda bu çifti atla\n",
        "            continue\n",
        "    return parsed_pairs\n",
        "\n",
        "df['Logcs'] = df['Logcs'].apply(parse_logcs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2dCrdm9CnSN"
      },
      "outputs": [],
      "source": [
        "# # Yeni filtre\n",
        "# def parse_logcs(logcs_str):\n",
        "#     # String ifadeyi parçalayarak liste haline getir\n",
        "#     try:\n",
        "#         # Kötü niyetli kodların çalıştırılmasını önlemek için güvenli bir yol kullanılıyor\n",
        "#         logcs_list = eval(logcs_str, {'__builtins__': None}, {})\n",
        "#     except:\n",
        "#         logcs_list = []\n",
        "\n",
        "#     # 0 değerlerinin sayısını hesapla\n",
        "#     zero_count = sum(1 for _, value in logcs_list if value == 0)\n",
        "#     return zero_count\n",
        "\n",
        "# # Yeni bir sütun ekleyerek her satır için 0 değerlerinin sayısını hesapla\n",
        "# df['Zero_Count'] = df['Logcs'].apply(parse_logcs)\n",
        "\n",
        "# # 1'den fazla 0 değerine sahip olanları at\n",
        "# df = df[df['Zero_Count'] <= 1]\n",
        "\n",
        "# # Decreases, Increases'tan fazla olanları at\n",
        "# df = df[df['Decreases'] <= df['Increases']]\n",
        "\n",
        "# # Total Values 8'den büyük olanları seç\n",
        "# df = df[df['Total Values'] > 8]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaX81EpB732p"
      },
      "outputs": [],
      "source": [
        "# #BlackList\n",
        "# import numpy as np\n",
        "# anomalies_path = \"/content/drive/MyDrive/LSTM_forecasting/anomalies.npy\"\n",
        "# loaded_anomalies = np.load(anomalies_path)\n",
        "# df_filtered = df[~df['Record ID'].isin(loaded_anomalies)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# DataFrame'inizi yükleyin (örnek: df = pd.read_csv(\"your_dataframe.csv\"))\n",
        "\n",
        "# Blacklist DataFrame'lerini yükle\n",
        "blacklist_dfs = []\n",
        "for i in range(2, 11):  # 2'den 8'e kadar (dahil) olan dosyalar için\n",
        "    path = f\"/content/drive/MyDrive/LSTM_forecasting/anomalies{i}.csv\"\n",
        "    blacklist_df = pd.read_csv(path, header=None)\n",
        "    blacklist_dfs.append(blacklist_df)\n",
        "\n",
        "# Tüm blacklist'leri birleştir\n",
        "combined_blacklist = pd.concat(blacklist_dfs)[0].tolist()\n",
        "\n",
        "# 'Record ID' sütununun olduğundan emin olun\n",
        "if 'Record ID' in df.columns:\n",
        "    # Blacklist'teki Record ID'leri içermeyen satırları filtrele\n",
        "    df = df[~df['Record ID'].isin(combined_blacklist)]\n",
        "else:\n",
        "    print(\"DataFrame does not have 'Record ID' column.\")\n",
        "\n",
        "# Filtrelenmiş DataFrame'in uzunluğunu yazdır\n",
        "len(df)\n"
      ],
      "metadata": {
        "id": "YmiJv2wscT3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhD81ViXthsd"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "original_columns = set(df.columns)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Organism', 'Food category'])\n",
        "all_columns = df.columns\n",
        "joblib.dump(all_columns, 'model_columns.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGkkkbQovps1"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "features_to_scale = ['Temperature (C)', 'Aw', 'pH']\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalizasyonu sadece belirli sütunlara uygula\n",
        "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "joblib.dump(scaler, 'scaler.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkPOyIR8tym7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Hariç tutulacak sütunlar\n",
        "exclude_columns = ['Record ID','Total Values', 'Increases', 'Decreases','Logcs', 'Zero_Count']\n",
        "\n",
        "# all_features listesini oluşturma\n",
        "all_features = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "# Veriyi hazırlama\n",
        "X = []  # Girdiler\n",
        "y = []  # Hedefler\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    logcs_data = row['Logcs']\n",
        "    # Tüm uygun özellikleri al\n",
        "    features = row[all_features].values\n",
        "    for i in range(len(logcs_data)-1):\n",
        "        X.append(np.concatenate(([logcs_data[i][1]], features)))  # current value and other features\n",
        "        y.append(logcs_data[i+1][1])  # next value\n",
        "\n",
        "X = np.array(X).reshape(-1, 1, len(X[0]))  # (samples, time steps, features)\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verilerinizi yükleyin veya oluşturun\n",
        "# X ve y verilerini bu aşamada hazırlayın\n",
        "\n",
        "# Verileri eğitim ve test kümesine bölün\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8jvRPmhu8FO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# X_RecordID ve Y_RecordID listelerini oluşturma\n",
        "X_RecordID = []\n",
        "Y_RecordID = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    record_id = row['Record ID']\n",
        "    logcs_data = row['Logcs']\n",
        "\n",
        "    # Tüm uygun özellikleri al\n",
        "    features = row[all_features].values\n",
        "    for i in range(len(logcs_data)-1):\n",
        "        X.append(np.concatenate(([logcs_data[i][1]], features)))\n",
        "        Y.append(logcs_data[i+1][1])  # Y listesini kullan\n",
        "\n",
        "        # RecordID'leri ekleme\n",
        "        X_RecordID.append(record_id)\n",
        "        Y_RecordID.append(record_id)\n",
        "\n",
        "# Numpy dizilerine dönüştürme\n",
        "X = np.array(X).reshape(-1, 1, len(X[0]))\n",
        "Y = np.array(Y).reshape(-1, 1)  # Y dizisini numpy array'e çevirme\n",
        "X_RecordID = np.array(X_RecordID)\n",
        "Y_RecordID = np.array(Y_RecordID)\n",
        "\n",
        "# Veriyi eğitim ve test setlerine ayırma\n",
        "X_train, X_test, Y_train, Y_test, X_train_ids, X_test_ids, Y_train_ids, Y_test_ids = train_test_split(\n",
        "    X, Y, X_RecordID, Y_RecordID, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "5cr_pM9FVkAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FQbFqQI2dBT"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# LSTM modeli oluşturun\n",
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(4, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(layers.LSTM(4, return_sequences=True, name='lstm_output'))\n",
        "model.add(layers.Dense(1, name='dense_output'))  # Çıkış katmanı\n",
        "\n",
        "# Modeli derleyin\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')  # İstediğiniz bir optimizasyon ve loss fonksiyonu kullanabilirsiniz\n",
        "\n",
        "# Early Stopping callback'i oluşturun\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Modeli eğitin ve Early Stopping callback'ini kullanın\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Modelin performansını değerlendirin\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "\n",
        "# Tahminler yapın\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Eğitim ve doğrulama kayıplarını görselleştirin\n",
        "plt.plot(history.history['val_loss'], label='Train Loss')\n",
        "plt.plot(history.history['loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaqKsQOMwCAq"
      },
      "outputs": [],
      "source": [
        "model.save(\"one_step_ahead.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1NqMJii21pA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Tahminleri ve gerçek değerleri alın\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Linear Regression modelini oluşturun ve eğitin\n",
        "regression_model = LinearRegression()\n",
        "y_test = y_test.reshape(-1, 1)  # Yeniden şekillendirme\n",
        "predictions = predictions.reshape(-1, 1)  # Yeniden şekillendirme\n",
        "regression_model.fit(y_test, predictions)\n",
        "\n",
        "# Gerçek değerlerin minimum ve maksimum değerlerini alın\n",
        "min_val = min(np.min(y_test), np.min(predictions))\n",
        "max_val = max(np.max(y_test), np.max(predictions))\n",
        "\n",
        "# Doğruyu çizmek için tahminler oluşturun\n",
        "x_values = np.linspace(min_val, max_val, 100)\n",
        "y_values = regression_model.predict(x_values.reshape(-1, 1))\n",
        "\n",
        "# Scatter plot oluşturun\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, predictions, alpha=0.5)\n",
        "plt.plot(x_values, y_values, color='red', linewidth=2, label='Regression Line')\n",
        "\n",
        "# X=Y doğrusunu çizmek için\n",
        "plt.plot([min_val, max_val], [min_val, max_val], color='blue', linewidth=2, label='X=Y Line')\n",
        "\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('Values vs. Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxBvmQSw00SF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Anomali tespiti için hata eşiği belirleme\n",
        "# hata_esigi = 2.3    # Bu değeri ihtiyaçlarınıza göre ayarlayın\n",
        "\n",
        "# predictions = model.predict(X_train)\n",
        "\n",
        "\n",
        "# # Anomali tespiti\n",
        "# anomali_maskesi = np.abs(y_train.reshape(-1) - predictions.reshape(-1)) > hata_esigi\n",
        "\n",
        "# # Anomali olarak tespit edilen örneklerin RecordID'lerini alın\n",
        "# anomali_record_ids = X_train_ids[anomali_maskesi]\n",
        "\n",
        "# # Bu RecordID'leri saklayın\n",
        "\n",
        "\n",
        "# # Bu RecordID'leri saklayın\n",
        "# # Bu noktada anomali_record_ids, anomalilerin RecordID'lerini içeren bir dizi\n",
        "# anomali_record_ids\n",
        "# anomali_record_ids_unique_train = np.unique(anomali_record_ids)\n",
        "\n",
        "# # Model ile test seti üzerinde tahmin yapma\n",
        "# predictions_test = model.predict(X_test)\n",
        "\n",
        "# # Test seti üzerinde anomali tespiti\n",
        "# anomali_maskesi_test = np.abs(y_test.reshape(-1) - predictions_test.reshape(-1)) > hata_esigi\n",
        "\n",
        "# # Anomali olarak tespit edilen örneklerin RecordID'lerini alın\n",
        "# anomali_record_ids_test = X_test_ids[anomali_maskesi_test]\n",
        "\n",
        "# # Benzersiz (unique) RecordID'leri alın\n",
        "# anomali_record_ids_unique_test = np.unique(anomali_record_ids_test)\n",
        "\n",
        "\n",
        "# anomalies = np.concatenate([anomali_record_ids_unique_test, anomali_record_ids_unique_train])\n",
        "# anomalies = np.unique(anomalies)\n",
        "# len(anomalies)\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# # Anomalileri bir dosyaya kaydetme\n",
        "# np.save('anomalies.npy', anomalies)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyroIfAd106b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t5_Sd5E7LY9"
      },
      "outputs": [],
      "source": [
        "plt.show()\n",
        "model.save('/content/drive/MyDrive/Bitirme/models/one_step_ahead.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52zN3UiUpv5U"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMcYJ-D03Dll"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "predictions = model.predict(X_train)\n",
        "\n",
        "# Tahminlerin boyutunu kontrol edin ve düzeltin\n",
        "predictions = np.squeeze(predictions)\n",
        "\n",
        "# Ortalama Mutlak Hata (MAE)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
        "\n",
        "# Kök Ortalama Karesel Hata (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_train, predictions))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")\n",
        "\n",
        "# R² Skoru\n",
        "r2 = r2_score(y_train, predictions)\n",
        "print(f\"R² Score: {r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTffXqK1p9uL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "# Tahminlerin boyutunu kontrol edin ve düzeltin\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "predictions = np.squeeze(predictions)\n",
        "\n",
        "# Ortalama Mutlak Hata (MAE)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
        "\n",
        "# Kök Ortalama Karesel Hata (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")\n",
        "\n",
        "# R² Skoru\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(f\"R² Score: {r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygdNk-sRHgJV"
      },
      "source": [
        "## Forecasting One Step Ahead Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgED_noiJMGm"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "# # Önce, verilerinizin olduğu DataFrame'den gerekli sütunları çıkarmak için:\n",
        "# # df = pd.DataFrame(df)  # 'your_data' veri setinizi temsil etmelidir.\n",
        "\n",
        "# # exclude_columns dışındaki sütunları seçin\n",
        "# feature_columns = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "# # Hedef ve özellikleri hazırlayın\n",
        "\n",
        "# X = []\n",
        "# y_true = []\n",
        "\n",
        "# for index, row in df.iterrows():\n",
        "#     # 'Logcs' sütunundaki zaman ve değer çiftlerini çıkar\n",
        "#     logcs_values = [val for (time, val) in row['Logcs']]\n",
        "#     y_true.extend(logcs_values[1:])  # İlk değeri hariç tutuyoruz çünkü onu öğrenmek için kullanacağız\n",
        "\n",
        "#     for i in range(len(logcs_values) - 1):\n",
        "#         # Özellik vektörünü oluşturun\n",
        "#         features = row[feature_columns].values\n",
        "#         # Bir önceki Logcs değerini özelliklerle birleştirin\n",
        "#         X.append(np.concatenate(([logcs_values[i]], features)))\n",
        "\n",
        "# # Modelinizle tahminler yapın\n",
        "# y_pred = []\n",
        "\n",
        "# for x in X:\n",
        "#     # Veri türünü np.float32 olarak ayarlayın\n",
        "#     x_float32 = np.array(x, dtype=np.float32)\n",
        "\n",
        "#     # Girdiyi modelin beklediği forma dönüştürün\n",
        "#     x_reshaped = x_float32.reshape(1, 1, 20)  # Reshape to (1, 1, 20)\n",
        "#     # Modelden tahmin al\n",
        "#     predicted_logcs = model.predict(x_reshaped)\n",
        "#     y_pred.append(predicted_logcs.flatten()[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkRcbYNkOHNf"
      },
      "outputs": [],
      "source": [
        "# # R^2 skorunu hesaplayın\n",
        "# r2 = r2_score(y_true, y_pred)\n",
        "# print(f'R^2 Score: {r2}')\n",
        "\n",
        "\n",
        "# # MAE hesapla\n",
        "# mae = mean_absolute_error(y_true, y_pred)\n",
        "# print(f'Mean Absolute Error (MAE): {mae}')\n",
        "\n",
        "# # RMSE hesapla\n",
        "# rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "# print(f'Root Mean Squared Error (RMSE): {rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un5NlE2nPyLt"
      },
      "source": [
        "## Encoder Decoder Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERKhGXik8WWN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# df DataFrame'ini ilk olarak eğitim ve test setlerine ayır\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# df_train DataFrame'ini daha sonra eğitim ve doğrulama setlerine ayır\n",
        "df_train_partial, df_val = train_test_split(df_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0uY5H3-88vN"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Gma1A-H0TX"
      },
      "source": [
        "### Zaman araliklari girilerek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nfx_9F3l7tXp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def prepare_data(df):\n",
        "    X, y, record_ids = [], [], []\n",
        "    exclude_columns = ['Record ID', 'Total Values', 'Increases', 'Decreases', 'Logcs', \"Zero_Count\"]\n",
        "\n",
        "    # all_features listesini oluşturma\n",
        "    all_features = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        logcs_data = row['Logcs']\n",
        "        features = row[all_features].values\n",
        "\n",
        "        if len(logcs_data) > 5:\n",
        "            X_input = []\n",
        "            first_time_value = logcs_data[0][0]  # İlk zaman değerini referans al\n",
        "            for i in range(5):\n",
        "                time_diff = logcs_data[i][0] - first_time_value\n",
        "                X_input.extend([time_diff, logcs_data[i][1]])\n",
        "                X_input.extend(features)\n",
        "\n",
        "            y_output = [logcs_data[i][1] for i in range(5, len(logcs_data))]\n",
        "\n",
        "            X.append(X_input)\n",
        "            y.append(y_output)\n",
        "            record_ids.append(row['Record ID'])  # Her veri noktası için Record ID'yi sakla\n",
        "\n",
        "    # Girdi boyutunu düzeltme\n",
        "    feature_len = 2 + len(all_features)  # Her zaman adımı için zaman farkı ve değer\n",
        "    time_steps = 5\n",
        "    return np.array(X).reshape(-1, time_steps, feature_len), np.array(y, dtype=object), record_ids\n",
        "\n",
        "# Veri hazırlama\n",
        "X_train, y_train, record_ids_train = prepare_data(df_train_partial)\n",
        "X_val, y_val, record_ids_val = prepare_data(df_val)\n",
        "X_test, y_test, record_ids_test = prepare_data(df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vN3lHQ9NGC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
        "import numpy as np\n",
        "\n",
        "# Maksimum çıktı uzunluğunu hesapla\n",
        "max_output_length = max([len(yi) for yi in y_train])\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(5, len(X_train[0][0])))  # 5 zaman adımı, özellik sayısı\n",
        "encoder_lstm = LSTM(50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = RepeatVector(max_output_length)(encoder_outputs)  # Burada encoder çıktısını tekrarla\n",
        "decoder_lstm = LSTM(50, return_sequences=True, return_state=False)\n",
        "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = TimeDistributed(Dense(1))  # Her zaman adımı için bir tahmin\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model = Model(inputs=encoder_inputs, outputs=decoder_outputs)\n",
        "\n",
        "# Model Derleme\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Model Özeti\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JikJ_vrv01u2"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "best_val_loss = np.inf\n",
        "best_weights = None\n",
        "\n",
        "epochs = 12  # Toplam epoch sayısı\n",
        "train_losses = []\n",
        "val_losses = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjnPRey39TaU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    # Eğitim döngüsü\n",
        "    for i in range(len(X_train)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_train[i:i+1], training=True)\n",
        "            train_loss = loss_fn(y_train[i], y_pred[0, :len(y_train[i]), 0])\n",
        "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        total_train_loss += train_loss.numpy()\n",
        "\n",
        "    # Doğrulama döngüsü\n",
        "    for i in range(len(X_val)):\n",
        "        # X_val[i:i+1] dizisini tensöre dönüştür\n",
        "        X_val_tensor = tf.convert_to_tensor(X_val[i:i+1], dtype=tf.float32)\n",
        "        y_pred_val = model.predict(X_val_tensor)\n",
        "\n",
        "        # y_val[i] dizisini tensöre dönüştür\n",
        "        y_val_tensor = tf.convert_to_tensor(y_val[i], dtype=tf.float32)\n",
        "\n",
        "        # Tahmin ve gerçek değerlerin boyutlarını eşitle\n",
        "        min_length = min(y_pred_val.shape[1], len(y_val[i]))\n",
        "        val_loss = loss_fn(y_val_tensor[:min_length], y_pred_val[0, :min_length, 0])\n",
        "\n",
        "        total_val_loss += val_loss.numpy()\n",
        "\n",
        "    # Ortalama kayıpları hesapla ve kaydet\n",
        "    avg_train_loss = total_train_loss / len(X_train)\n",
        "    avg_val_loss = total_val_loss / len(X_val)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_weights = model.get_weights()\n",
        "\n",
        "    print(f\"Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "\n",
        "if best_weights:\n",
        "    model.set_weights(best_weights)\n",
        "\n",
        "# Kayıpları grafik üzerinde görselleştirme\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Train & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OBatZOUNf6l"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Train & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rf9DuGp9wYH"
      },
      "outputs": [],
      "source": [
        "# Test veri seti üzerinde tahmin yapma\n",
        "y_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYhCNTOhM4tT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse_values = []\n",
        "    mae_values = []\n",
        "    r2_values = []\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        true_values = np.array(y_true[i])\n",
        "        pred_values = y_pred[i, :len(y_true[i]), 0]\n",
        "\n",
        "        mse = mean_squared_error(true_values, pred_values)\n",
        "        mae = mean_absolute_error(true_values, pred_values)\n",
        "\n",
        "        mse_values.append(mse)\n",
        "        mae_values.append(mae)\n",
        "\n",
        "        # R2 skorunu yalnızca birden fazla değeri olan örnekler için hesap\n",
        "        if len(true_values) > 1:\n",
        "            r2 = r2_score(true_values, pred_values)\n",
        "            r2_values.append(r2)\n",
        "\n",
        "    return np.mean(mse_values), np.mean(mae_values), np.mean(r2_values) if r2_values else np.nan\n",
        "\n",
        "# Hata metriklerini hesaplama\n",
        "mse, mae, r2 = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", \"{:.2f}\".format(mse))\n",
        "print(\"Mean Absolute Error:\", \"{:.2f}\".format(mae))\n",
        "print(\"R2 Score:\", \"{:.2f}\".format(r2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co90Hhc41kiT"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytH_wEIS479p"
      },
      "source": [
        "## Anomaly Detection v2 BlackList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYTFNAo3peeR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_metrics_by_id(y_true, y_pred, record_ids, r2_threshold):\n",
        "    metrics_by_id = {}\n",
        "    blacklist = []\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        record_id = record_ids[i]\n",
        "        true_values = np.array(y_true[i])\n",
        "        pred_values = y_pred[i, :len(y_true[i]), 0]\n",
        "\n",
        "        mse = mean_squared_error(true_values, pred_values)\n",
        "        mae = mean_absolute_error(true_values, pred_values)\n",
        "        r2 = r2_score(true_values, pred_values) if len(true_values) > 1 else np.nan\n",
        "\n",
        "        metrics_by_id[record_id] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
        "\n",
        "        if r2 < r2_threshold:\n",
        "            blacklist.append(record_id)\n",
        "\n",
        "    return metrics_by_id, blacklist\n",
        "\n",
        "total_sample=len(y_test)\n",
        "# Hata metriklerini ve karalisteyi hesapla\n",
        "r2_threshold = 0.3  # Örnek bir eşik değer\n",
        "y_pred = model.predict(X_test)\n",
        "metrics_by_id, blacklist = calculate_metrics_by_id(y_test, y_pred, record_ids_test, r2_threshold)\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "total_sample+=len(y_train)\n",
        "metrics_by_id2, blacklist2 = calculate_metrics_by_id(y_train, y_pred, record_ids_train, r2_threshold)\n",
        "\n",
        "filtered_sample_count = total_sample - (len(blacklist) + len(blacklist2) )\n",
        "filtered_sample_count\n",
        "blacklistt = blacklist+blacklist2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAllwlusMudq"
      },
      "outputs": [],
      "source": [
        "print(filtered_sample_count)\n",
        "print(len(blacklistt))\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujfw1KRN4dGw"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('anomalies11.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for record_id in blacklistt:\n",
        "        writer.writerow([record_id])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpmlaZjL63y4"
      },
      "outputs": [],
      "source": [
        "filtered_sample_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvRXZmrWppQO"
      },
      "outputs": [],
      "source": [
        "len(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0GTZwpIO2pp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "selected_indices = random.sample(range(len(y_test)),25)\n",
        "\n",
        "for index in selected_indices:\n",
        "    plt.figure(figsize=(8, 4))  # Grafik boyutu\n",
        "    plt.plot(y_test[index], marker='o', label='y_test')\n",
        "\n",
        "    # y_pred verisini y_test verisinin uzunluğu\n",
        "    y_pred_trimmed = y_pred[index][:len(y_test[index])]\n",
        "    plt.plot(y_pred_trimmed, marker='x', label='y_pred')\n",
        "\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title(f'Sample {index + 1}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYVXkgQDPA7v"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krs5VXobO5gZ"
      },
      "outputs": [],
      "source": [
        "model.save('encoder_decoder_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-EHv_GnZkq8"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics_and_blacklist(df, y_true, y_pred, r2_threshold):\n",
        "    mse_values = []\n",
        "    mae_values = []\n",
        "    r2_values = []\n",
        "    blacklist = []\n",
        "\n",
        "    for i, (index, row) in enumerate(df.iterrows()):\n",
        "        record_id = row['Record ID']\n",
        "        true_values = np.array(y_true[i])\n",
        "        pred_values = y_pred[i, :len(y_true[i]), 0]\n",
        "\n",
        "        if len(true_values) > len(pred_values):\n",
        "            true_values = true_values[:len(pred_values)]\n",
        "\n",
        "        mse = mean_squared_error(true_values, pred_values)\n",
        "        mae = mean_absolute_error(true_values, pred_values)\n",
        "        mse_values.append(mse)\n",
        "        mae_values.append(mae)\n",
        "\n",
        "        if len(true_values) > 1:\n",
        "            r2 = r2_score(true_values, pred_values)\n",
        "            r2_values.append(r2)\n",
        "            if r2 < r2_threshold:\n",
        "                blacklist.append(record_id)\n",
        "\n",
        "    return np.mean(mse_values), np.mean(mae_values), np.mean(r2_values), blacklist\n",
        "\n",
        "# Hata metriklerini ve karalisteyi hesapla\n",
        "r2_threshold = 0.5  # Örnek bir eşik değer\n",
        "mse, mae, r2, blacklist = calculate_metrics_and_blacklist(df, y_train, y_pred, r2_threshold)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"R2 Score:\", r2)\n",
        "print(\"Blacklisted Record IDs:\", blacklist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5-00Ctlj1R7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1kxjvkkv7IETuYel4Uh51mA5CVs0EWYes",
      "authorship_tag": "ABX9TyPitGlJFSYd+1tgLJvhODQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}